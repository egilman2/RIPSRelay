{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNOaZZpZUYJqTqtxAPYH45e",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/egilman2/RIPSRelay/blob/main/finetune_pubchem_light_classification_commented.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9dO99mO2TZNa"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import torch\n",
        "from torch import nn\n",
        "import args\n",
        "import os\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import random\n",
        "from pytorch_lightning.loggers import TensorBoardLogger\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.utilities import rank_zero_warn, rank_zero_only, seed\n",
        "from tokenizer.tokenizer import MolTranBertTokenizer\n",
        "from fast_transformers.masking import LengthMask as LM\n",
        "from rotate_attention.rotate_builder import RotateEncoderBuilder as rotate_builder\n",
        "from fast_transformers.feature_maps import GeneralizedRandomFeatures\n",
        "from functools import partial\n",
        "from apex import optimizers\n",
        "import subprocess\n",
        "from argparse import ArgumentParser, Namespace\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.stats import pearsonr\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.metrics import accuracy_score, roc_curve, auc\n",
        "from torch.utils.data import DataLoader\n",
        "from rdkit import Chem\n",
        "#from pytorch_lightning.plugins.ddp_plugin import DDPPlugin\n",
        "#from pytorch_lightning.plugins.sharded_plugin import DDPShardedPlugin\n",
        "\n",
        "def normalize_smiles(smi, canonical, isomeric):\n",
        "    try:\n",
        "        normalized = Chem.MolToSmiles(\n",
        "            Chem.MolFromSmiles(smi), canonical=canonical, isomericSmiles=isomeric\n",
        "        ) # Chem.MolFromSmiles takes smi which is a smile string to a molecule and then casts it to a canonical smile with Chem.MolToSmiles and canonical=canonical\n",
        "        # For basic examples of these rdkit functions, go to https://www.herongyang.com/Cheminformatics/RDKit-rdkit-Chem-rdmolfiles-SMILES-Format.html\n",
        "    except:\n",
        "        normalized = None\n",
        "    return normalized\n",
        "\n",
        "class LightningModule(pl.LightningModule):\n",
        "\n",
        "    def __init__(self, config, tokenizer):\n",
        "        super(LightningModule, self).__init__()\n",
        "\n",
        "        self.config = config\n",
        "        self.hparams = config\n",
        "        self.mode = config.mode\n",
        "        self.save_hyperparameters(config)\n",
        "        self.tokenizer=tokenizer\n",
        "        self.min_loss = {\n",
        "            self.hparams.measure_name + \"min_valid_loss\": torch.finfo(torch.float32).max,\n",
        "            self.hparams.measure_name + \"min_epoch\": 0,\n",
        "        } # Dictionary which will hold minimum validation loss info\n",
        "\n",
        "        # Word embeddings layer\n",
        "        n_vocab, d_emb = len(tokenizer.vocab), config.n_embd # Recall that d_emb is the dimension of the token embeddings after they pop out of the transformer\n",
        "        # input embedding stem\n",
        "        builder = rotate_builder.from_kwargs(\n",
        "            n_layers=config.n_layer,\n",
        "            n_heads=config.n_head,\n",
        "            query_dimensions=config.n_embd//config.n_head,\n",
        "            value_dimensions=config.n_embd//config.n_head,\n",
        "            feed_forward_dimensions=config.n_embd,\n",
        "            attention_type='linear',\n",
        "            feature_map=partial(GeneralizedRandomFeatures, n_dims=config.num_feats),\n",
        "            activation='gelu',\n",
        "            ) # Look at https://fast-transformers.github.io/api_docs/fast_transformers/builders/transformer_builders.html to see how the base builder works here.\n",
        "              # This really just builds the transformer architecture (ie all the stuff around the attention mechanisms\n",
        "        self.pos_emb = None\n",
        "        self.tok_emb = nn.Embedding(n_vocab, config.n_embd)\n",
        "        self.drop = nn.Dropout(config.d_dropout)\n",
        "        ## transformer\n",
        "        self.blocks = builder.get() # From the Base which Rotate extends\n",
        "        self.lang_model = self.lm_layer(config.n_embd, n_vocab) # Language model layer (simply 2-layer FFNN) which seems to map from size of token embedding to number of tokens in the vocab\n",
        "        self.train_config = config\n",
        "        #if we are starting from scratch set seeds\n",
        "        #########################################\n",
        "        # protein_emb_dim, smiles_embed_dim, dims=dims, dropout=0.2):\n",
        "        #########################################\n",
        "\n",
        "        self.fcs = []  # nn.ModuleList(), never used\n",
        "        self.loss = torch.nn.CrossEntropyLoss() # Of course using classification here\n",
        "\n",
        "        self.net = self.Net(\n",
        "            config.n_embd, self.hparams.num_classes, dims=config.dims, dropout=config.dropout,\n",
        "        ) # This is the MLP attached at the end which takes in word embeddings as input\n",
        "\n",
        "\n",
        "    # Net is EXACTLY as described in the paper, very straightforward\n",
        "    class Net(nn.Module):\n",
        "        dims = [150, 50, 50, 2]\n",
        "\n",
        "\n",
        "        def __init__(self, smiles_embed_dim, num_classes, dims=dims, dropout=0.2):\n",
        "            super().__init__()\n",
        "            self.desc_skip_connection = True\n",
        "            self.fcs = []  # nn.ModuleList()\n",
        "            print('dropout is {}'.format(dropout))\n",
        "\n",
        "            self.fc1 = nn.Linear(smiles_embed_dim, smiles_embed_dim)\n",
        "            self.dropout1 = nn.Dropout(dropout)\n",
        "            self.relu1 = nn.GELU()\n",
        "            self.fc2 = nn.Linear(smiles_embed_dim, smiles_embed_dim)\n",
        "            self.dropout2 = nn.Dropout(dropout)\n",
        "            self.relu2 = nn.GELU()\n",
        "            self.final = nn.Linear(smiles_embed_dim, num_classes) #classif\n",
        "\n",
        "        def forward(self, smiles_emb):\n",
        "            x_out = self.fc1(smiles_emb)\n",
        "            x_out = self.dropout1(x_out)\n",
        "            x_out = self.relu1(x_out)\n",
        "\n",
        "            if self.desc_skip_connection is True:\n",
        "                x_out = x_out + smiles_emb\n",
        "\n",
        "            z = self.fc2(x_out)\n",
        "            z = self.dropout2(z)\n",
        "            z = self.relu2(z)\n",
        "            if self.desc_skip_connection is True:\n",
        "                z = self.final(z + x_out)\n",
        "            else:\n",
        "                z = self.final(z)\n",
        "\n",
        "            #z = self.layers(smiles_emb)\n",
        "            return z\n",
        "\n",
        "    # NEED TO FIGURE OUT EXACTLY WHERE LM_LAYER FITS IN THIS PICTURE\n",
        "    class lm_layer(nn.Module):\n",
        "        def __init__(self, n_embd, n_vocab):\n",
        "            super().__init__()\n",
        "            self.embed = nn.Linear(n_embd, n_embd)\n",
        "            self.ln_f = nn.LayerNorm(n_embd)\n",
        "            self.head = nn.Linear(n_embd, n_vocab, bias=False)\n",
        "        def forward(self, tensor):\n",
        "            tensor = self.embed(tensor)\n",
        "            tensor = F.gelu(tensor)\n",
        "            tensor = self.ln_f(tensor)\n",
        "            tensor = self.head(tensor)\n",
        "            return tensor\n",
        "\n",
        "    # Returns the loss for our downstream tasks along with the predictions the model gave and the targets\n",
        "    def get_loss(self, smiles_emb, measures):\n",
        "        z_pred = self.net.forward(smiles_emb).squeeze()\n",
        "        measures = measures.long()\n",
        "        #print('z_pred:', z_pred.shape)\n",
        "        #print('measures:', measures.shape)\n",
        "        return self.loss(z_pred, measures), z_pred, measures\n",
        "\n",
        "    # Exactly what you think it is for both the next two functions\n",
        "    def on_save_checkpoint(self, checkpoint):\n",
        "        #save RNG states each time the model and states are saved\n",
        "        out_dict = dict()\n",
        "        out_dict['torch_state']=torch.get_rng_state()\n",
        "        out_dict['cuda_state']=torch.cuda.get_rng_state()\n",
        "        if np:\n",
        "            out_dict['numpy_state']=np.random.get_state()\n",
        "        if random:\n",
        "            out_dict['python_state']=random.getstate()\n",
        "        checkpoint['rng'] = out_dict\n",
        "\n",
        "    def on_load_checkpoint(self, checkpoint):\n",
        "        #load RNG states each time the model and states are loaded from checkpoint\n",
        "        rng = checkpoint['rng']\n",
        "        for key, value in rng.items():\n",
        "            if key =='torch_state':\n",
        "                torch.set_rng_state(value)\n",
        "            elif key =='cuda_state':\n",
        "                torch.cuda.set_rng_state(value)\n",
        "            elif key =='numpy_state':\n",
        "                np.random.set_state(value)\n",
        "            elif key =='python_state':\n",
        "                random.setstate(value)\n",
        "            else:\n",
        "                print('unrecognized state')\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, (nn.Linear, nn.Embedding)):\n",
        "            module.weight.data.normal_(mean=0.0, std=0.02) # initializes weight matrix with this distribution\n",
        "            if isinstance(module, nn.Linear) and module.bias is not None:\n",
        "                module.bias.data.zero_() # Makes biases 0\n",
        "        elif isinstance(module, nn.LayerNorm):\n",
        "            module.bias.data.zero_() # Makes biases 0\n",
        "            module.weight.data.fill_(1.0) # Fixes weight matrix for LayerNorm as 1's\n",
        "\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        # separate out all parameters to those that will and won't experience regularizing weight decay\n",
        "        decay = set()\n",
        "        no_decay = set()\n",
        "        whitelist_weight_modules = (torch.nn.Linear, ) # whitelisted do decay\n",
        "        blacklist_weight_modules = (torch.nn.LayerNorm, torch.nn.Embedding) # blacklisted do not decay\n",
        "        for mn, m in self.named_modules(): # mn, m: module name, module\n",
        "            for pn, p in m.named_parameters(): # pn, p: parameter name, parameter\n",
        "                fpn = '%s.%s' % (mn, pn) if mn else pn # full param name\n",
        "\n",
        "                if pn.endswith('bias'):\n",
        "                    # all biases will not be decayed\n",
        "                    no_decay.add(fpn)\n",
        "                elif pn.endswith('weight') and isinstance(m, whitelist_weight_modules):\n",
        "                    # weights of whitelist modules will be weight decayed\n",
        "                    decay.add(fpn)\n",
        "                elif pn.endswith('weight') and isinstance(m, blacklist_weight_modules):\n",
        "                    # weights of blacklist modules will NOT be weight decayed\n",
        "                    no_decay.add(fpn)\n",
        "\n",
        "\n",
        "        if self.pos_emb != None:\n",
        "            no_decay.add('pos_emb') # Positional embeddings will not be decayed\n",
        "\n",
        "        # validate that we considered every parameter and the sets of decay and no decay are are a disjoint union of all parameters\n",
        "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
        "        inter_params = decay & no_decay\n",
        "        union_params = decay | no_decay\n",
        "        assert len(inter_params) == 0, \"parameters %s made it into both decay/no_decay sets!\" % (str(inter_params), )\n",
        "        assert len(param_dict.keys() - union_params) == 0, \"parameters %s were not separated into either decay/no_decay set!\" \\\n",
        "                                                    % (str(param_dict.keys() - union_params), )\n",
        "\n",
        "        # create the pytorch optimizer object\n",
        "        optim_groups = [\n",
        "            {\"params\": [param_dict[pn] for pn in sorted(list(decay))], \"weight_decay\": 0.0},\n",
        "            {\"params\": [param_dict[pn] for pn in sorted(list(no_decay))], \"weight_decay\": 0.0},\n",
        "        ]\n",
        "        if self.hparams.measure_name == 'r2':\n",
        "            betas = (0.9, 0.999)\n",
        "        else:\n",
        "            betas = (0.9, 0.99)\n",
        "        print('betas are {}'.format(betas))\n",
        "        learning_rate = self.train_config.lr_start * self.train_config.lr_multiplier\n",
        "        optimizer = optimizers.FusedLAMB(optim_groups, lr=learning_rate, betas=betas) # Should learn more theory behind FusedLAMB\n",
        "        return optimizer\n",
        "\n",
        "    def training_step(self, batch, batch_idx): # batch[0] holds the index, batch[1] holds the mask, batch[-1] holds all the targets\n",
        "        idx = batch[0]\n",
        "        mask = batch[1]\n",
        "        targets = batch[-1]\n",
        "\n",
        "        loss = 0\n",
        "        loss_tmp = 0\n",
        "        b, t = idx.size() # never used\n",
        "        token_embeddings = self.tok_emb(idx) # each index maps to a (learnable) vector\n",
        "        x = self.drop(token_embeddings) # The dropout\n",
        "        x = self.blocks(x, length_mask=LM(mask.sum(-1))) # the blocks were built previously and are the transformer (excluding the attention). So this line is passing the x through the transformer with\n",
        "        token_embeddings = x\n",
        "        input_mask_expanded = mask.unsqueeze(-1).expand(token_embeddings.size()).float() # This is done so that the masking can take place in the next line\n",
        "        sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1) # This line is where the masking happens\n",
        "        sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9) # Will clamp the sum_mask because we divide by it in the next line\n",
        "        loss_input = sum_embeddings / sum_mask # Is this for the linear version of attention as opposed to what is typically done with softmax. NEED TO FIGURE THIS OUT\n",
        "        loss, pred, actual = self.get_loss(loss_input, targets) # Using the get_loss function from above\n",
        "\n",
        "        self.log('train_loss', loss, on_step=True)\n",
        "\n",
        "        logs = {\"train_loss\": loss}\n",
        "\n",
        "        return {\"loss\": loss}\n",
        "\n",
        "    # Very similar to above function\n",
        "    def validation_step(self, val_batch, batch_idx, dataset_idx):\n",
        "        idx = val_batch[0]\n",
        "        mask = val_batch[1]\n",
        "        targets = val_batch[-1]\n",
        "\n",
        "        loss = 0\n",
        "        loss_tmp = 0\n",
        "        b, t = idx.size() # never used\n",
        "        token_embeddings = self.tok_emb(idx) # each index maps to a (learnable) vector\n",
        "        x = self.drop(token_embeddings)\n",
        "        x = self.blocks(x, length_mask=LM(mask.sum(-1)))\n",
        "        token_embeddings = x\n",
        "        input_mask_expanded = mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
        "        sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
        "        sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
        "        loss_input = sum_embeddings / sum_mask\n",
        "        loss, pred, actual = self.get_loss(loss_input, targets)\n",
        "        self.log('train_loss', loss, on_step=True)\n",
        "        return {\n",
        "            \"val_loss\": loss,\n",
        "            \"pred\": pred.detach(),\n",
        "            \"actual\": actual.detach(),\n",
        "            \"dataset_idx\": dataset_idx,\n",
        "        } # Only difference between this and training step is this return statement\n",
        "\n",
        "    def validation_epoch_end(self, outputs):\n",
        "        # results_by_dataset = self.split_results_by_dataset(outputs)\n",
        "        tensorboard_logs = {}\n",
        "        for dataset_idx, batch_outputs in enumerate(outputs):\n",
        "            dataset = self.hparams.dataset_names[dataset_idx] # recall self.hparams was set equal to config, in the end dataset will just be the name of the dataset\n",
        "            #print(\"x_val_loss:\", batch_outputs[0][\"val_loss\"])\n",
        "            avg_loss = torch.stack([x[\"val_loss\"] for x in batch_outputs]).mean() # stack val_loss for the batches then compute the mean of those numbers\n",
        "            preds = torch.cat([x[\"pred\"] for x in batch_outputs]) # Get the predictions the model outputs together\n",
        "            actuals = torch.cat([x[\"actual\"] for x in batch_outputs]) # Get the targets\n",
        "            val_loss = self.loss(preds, actuals)\n",
        "\n",
        "            # Stuff here is moved to the CPU\n",
        "            actuals_cpu = actuals.detach().cpu().numpy()\n",
        "            #preds_cpu = preds.detach().cpu().numpy() #preds_cpu is logits\n",
        "            preds_cpu = F.softmax(preds, dim=1).cpu().numpy()\n",
        "\n",
        "            # Work with ROC curve statistics\n",
        "            preds_cpu = preds_cpu[:, 1]\n",
        "            fpr, tpr, threshold = roc_curve(actuals_cpu, preds_cpu)\n",
        "            roc_auc = auc(fpr, tpr)\n",
        "\n",
        "            print(dataset,':rocauc:', roc_auc)\n",
        "            y_pred = np.where(preds_cpu >= 0.5, 1, 0)\n",
        "            accuracy = accuracy_score(actuals_cpu, y_pred)\n",
        "            # Logging below can be looked into some other time\n",
        "            tensorboard_logs.update(\n",
        "                {\n",
        "                    # dataset + \"_avg_val_loss\": avg_loss,\n",
        "                    self.hparams.measure_name + \"_\" + dataset + \"_loss\": val_loss,\n",
        "                    self.hparams.measure_name + \"_\" + dataset + \"_acc\": accuracy,\n",
        "                    self.hparams.measure_name + \"_\" + dataset + \"_rocauc\": roc_auc,\n",
        "                }\n",
        "            )\n",
        "\n",
        "        if (\n",
        "            tensorboard_logs[self.hparams.measure_name + \"_valid_loss\"]\n",
        "            < self.min_loss[self.hparams.measure_name + \"min_valid_loss\"]\n",
        "        ):\n",
        "            self.min_loss[self.hparams.measure_name + \"min_valid_loss\"] = tensorboard_logs[\n",
        "                self.hparams.measure_name + \"_valid_loss\"\n",
        "            ]\n",
        "            self.min_loss[self.hparams.measure_name + \"min_test_loss\"] = tensorboard_logs[\n",
        "                self.hparams.measure_name + \"_test_loss\"\n",
        "            ]\n",
        "            self.min_loss[self.hparams.measure_name + \"min_epoch\"] = self.current_epoch\n",
        "            self.min_loss[self.hparams.measure_name + \"max_valid_rocauc\"] = tensorboard_logs[\n",
        "                self.hparams.measure_name + \"_valid_rocauc\"\n",
        "            ]\n",
        "            self.min_loss[self.hparams.measure_name + \"max_test_rocauc\"] = tensorboard_logs[\n",
        "                self.hparams.measure_name + \"_test_rocauc\"\n",
        "            ]\n",
        "\n",
        "\n",
        "\n",
        "        tensorboard_logs[self.hparams.measure_name + \"_min_valid_loss\"] = self.min_loss[\n",
        "            self.hparams.measure_name + \"min_valid_loss\"\n",
        "        ]\n",
        "        tensorboard_logs[self.hparams.measure_name + \"_min_test_loss\"] = self.min_loss[\n",
        "            self.hparams.measure_name + \"min_test_loss\"\n",
        "        ]\n",
        "        tensorboard_logs[self.hparams.measure_name + \"_max_test_rocauc\"] = self.min_loss[\n",
        "            self.hparams.measure_name + \"max_test_rocauc\"\n",
        "        ]\n",
        "        tensorboard_logs[self.hparams.measure_name + \"_max_valid_rocauc\"] = self.min_loss[\n",
        "            self.hparams.measure_name + \"max_valid_rocauc\"\n",
        "        ]\n",
        "\n",
        "\n",
        "\n",
        "        self.logger.log_metrics(tensorboard_logs, self.global_step)\n",
        "\n",
        "        for k in tensorboard_logs.keys():\n",
        "            self.log(k, tensorboard_logs[k])\n",
        "\n",
        "        print(\"Validation: Current Epoch\", self.current_epoch)\n",
        "        append_to_file(\n",
        "            os.path.join(self.hparams.results_dir, \"results_\" + str(self.hparams.run_id)+\".csv\"),\n",
        "            f\"{self.hparams.measure_name}, {self.current_epoch},\"\n",
        "            + f\"{tensorboard_logs[self.hparams.measure_name + '_valid_loss']},\"\n",
        "            + f\"{tensorboard_logs[self.hparams.measure_name + '_test_loss']},\"\n",
        "            + f\"{self.min_loss[self.hparams.measure_name + 'min_epoch']},\"\n",
        "            + f\"{self.min_loss[self.hparams.measure_name + 'max_valid_rocauc']},\"\n",
        "            + f\"{self.min_loss[self.hparams.measure_name + 'max_test_rocauc']}\",\n",
        "        )\n",
        "\n",
        "        return {\"avg_val_loss\": avg_loss}\n",
        "\n",
        "\n",
        "def get_dataset(data_root, filename,  dataset_len, aug, measure_name):\n",
        "    df = pd.read_csv(os.path.join(data_root, filename))\n",
        "    print(\"Length of dataset:\", len(df))\n",
        "    if dataset_len: # If it is not None ie if it is given to the function as an argument\n",
        "        df = df.head(dataset_len)\n",
        "        print(\"Warning entire dataset not used:\", len(df))\n",
        "    dataset = PropertyPredictionDataset(df,  measure_name, aug)\n",
        "    return dataset\n",
        "\n",
        "class PropertyPredictionDataset(torch.utils.data.Dataset): # Extends the well known Dataset class from torch\n",
        "    def __init__(self, df, measure_name, tokenizer=None, aug=True):\n",
        "        df = df[['smiles', measure_name]] # Make the only columns of the data frame be the smiles and measure_name\n",
        "        df = df.dropna() # Drop invalid entries\n",
        "        self.measure_name = measure_name\n",
        "        df['canonical_smiles'] = df['smiles'].apply(lambda smi: normalize_smiles(smi, canonical=True, isomeric=False)) # Look into isomeric\n",
        "        df_good = df.dropna(subset=['canonical_smiles'])  # TODO - Check why some rows are na\n",
        "\n",
        "        # Below is simply making it a normal dataframe without the invalid data\n",
        "        len_new = len(df_good)\n",
        "        print('Dropped {} invalid smiles'.format(len(df) - len_new))\n",
        "        self.df = df_good\n",
        "        self.df = self.df.reset_index(drop=True)\n",
        "\n",
        "    # Takes an index and returns (canonical_smiles, measures)\n",
        "    def __getitem__(self, index):\n",
        "        canonical_smiles = self.df.loc[index, 'canonical_smiles']\n",
        "        measures = self.df.loc[index, self.measure_name]\n",
        "        return canonical_smiles, measures\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "\n",
        "class PropertyPredictionDataModule(pl.LightningDataModule):\n",
        "    def __init__(self, hparams):\n",
        "        super(PropertyPredictionDataModule, self).__init__()\n",
        "        if type(hparams) is dict:\n",
        "            hparams = Namespace(**hparams) # if hparams is a dictionary, make it a namespace\n",
        "        self.hparams = hparams\n",
        "        self.smiles_emb_size = hparams.n_embd\n",
        "        self.tokenizer = MolTranBertTokenizer('bert_vocab.txt') # Some regex stuff here to tokenize, as shown in tokenizer.py\n",
        "        self.dataset_name = hparams.dataset_name\n",
        "\n",
        "    # DOES NOT take self as an argument\n",
        "    def get_split_dataset_filename(dataset_name, split):\n",
        "        return split + \".csv\"\n",
        "\n",
        "    def prepare_data(self):\n",
        "        print(\"Inside prepare_dataset\")\n",
        "        train_filename = PropertyPredictionDataModule.get_split_dataset_filename(self.dataset_name, \"train\")\n",
        "        valid_filename = PropertyPredictionDataModule.get_split_dataset_filename(self.dataset_name, \"valid\")\n",
        "        test_filename = PropertyPredictionDataModule.get_split_dataset_filename(self.dataset_name, \"test\")\n",
        "        # Essentially, hparams will be a namespace holding all relevant information about constructing the datasets\n",
        "        train_ds = get_dataset(\n",
        "            self.hparams.data_root,\n",
        "            train_filename,\n",
        "            self.hparams.train_dataset_length,\n",
        "            self.hparams.aug, # train has aug, test and valid do not\n",
        "            measure_name=self.hparams.measure_name,\n",
        "        )\n",
        "        val_ds = get_dataset(\n",
        "            self.hparams.data_root,\n",
        "            valid_filename,\n",
        "            self.hparams.eval_dataset_length,\n",
        "            aug=False,\n",
        "            measure_name=self.hparams.measure_name,\n",
        "        )\n",
        "        test_ds = get_dataset(\n",
        "            self.hparams.data_root,\n",
        "            test_filename,\n",
        "            self.hparams.eval_dataset_length,\n",
        "            aug=False,\n",
        "            measure_name=self.hparams.measure_name,\n",
        "        )\n",
        "\n",
        "        self.train_ds = train_ds\n",
        "        self.val_ds = [val_ds] + [test_ds] # Remember that they store the val_ds and test_ds in self.val_ds concatenated together\n",
        "\n",
        "        # print(\n",
        "        #     f\"Train dataset size: {len(self.train_ds)}, val: {len(self.val_ds1), len(self.val_ds2)}, test: {len(self.test_ds)}\"\n",
        "        # )\n",
        "\n",
        "    # Collate is passed into the val_dataloader and train_dataloader\n",
        "    def collate(self, batch):\n",
        "        tokens = self.tokenizer.batch_encode_plus([ smile[0] for smile in batch], padding=True, add_special_tokens=True) # Does the tokenization of each sequence in a batch\n",
        "        # Converts the input_ids to numbers (which represent them, lookup table style), the attention mask to numbers, and\n",
        "        return (torch.tensor(tokens['input_ids']), torch.tensor(tokens['attention_mask']), torch.tensor([smile[1] for smile in batch]))\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return [\n",
        "            DataLoader(\n",
        "                ds,\n",
        "                batch_size=self.hparams.batch_size,\n",
        "                num_workers=self.hparams.num_workers,\n",
        "                shuffle=False,\n",
        "                collate_fn=self.collate,\n",
        "            )\n",
        "            for ds in self.val_ds # Does it for each val_da and test_ds, recall the above\n",
        "        ]\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(\n",
        "            self.train_ds,\n",
        "            batch_size=self.hparams.batch_size,\n",
        "            num_workers=self.hparams.num_workers,\n",
        "            shuffle=True,\n",
        "            collate_fn=self.collate,\n",
        "        )\n",
        "\n",
        "\n",
        "\n",
        "class CheckpointEveryNSteps(pl.Callback):\n",
        "    \"\"\"\n",
        "        Save a checkpoint every N steps, instead of Lightning's default that checkpoints\n",
        "        based on validation loss.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, save_step_frequency=-1,\n",
        "        prefix=\"N-Step-Checkpoint\",\n",
        "        use_modelcheckpoint_filename=False,\n",
        "        ):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "        save_step_frequency: how often to save in steps\n",
        "        prefix: add a prefix to the name, only used if\n",
        "        use_modelcheckpoint_filename=False\n",
        "        \"\"\"\n",
        "        self.save_step_frequency = save_step_frequency\n",
        "        self.prefix = prefix\n",
        "        self.use_modelcheckpoint_filename = use_modelcheckpoint_filename\n",
        "    def on_batch_end(self, trainer: pl.Trainer, _):\n",
        "        \"\"\" Check if we should save a checkpoint after every train batch \"\"\"\n",
        "        epoch = trainer.current_epoch\n",
        "        global_step = trainer.global_step\n",
        "\n",
        "        if global_step % self.save_step_frequency == 0 and self.save_step_frequency > 10:\n",
        "\n",
        "            if self.use_modelcheckpoint_filename:\n",
        "                filename = trainer.checkpoint_callback.filename\n",
        "            else:\n",
        "                filename = f\"{self.prefix}_{epoch}_{global_step}.ckpt\"\n",
        "                #filename = f\"{self.prefix}.ckpt\"\n",
        "            ckpt_path = os.path.join(trainer.checkpoint_callback.dirpath, filename)\n",
        "            trainer.save_checkpoint(ckpt_path)\n",
        "\n",
        "class ModelCheckpointAtEpochEnd(pl.Callback):\n",
        "    def on_epoch_end(self, trainer, pl_module):\n",
        "        metrics = trainer.callback_metrics\n",
        "        metrics['epoch'] = trainer.current_epoch\n",
        "        if trainer.disable_validation:\n",
        "            trainer.checkpoint_callback.on_validation_end(trainer, pl_module)\n",
        "\n",
        "\n",
        "def append_to_file(filename, line):\n",
        "    with open(filename, \"a\") as f:\n",
        "        f.write(line + \"\\n\")\n",
        "\n",
        "def main():\n",
        "    margs = args.parse_args() # margs holds all the configuration details\n",
        "    print(\"Using \" + str(\n",
        "        torch.cuda.device_count()) + \" GPUs---------------------------------------------------------------------\")\n",
        "    pos_emb_type = 'rot'\n",
        "    print('pos_emb_type is {}'.format(pos_emb_type))\n",
        "\n",
        "    run_name_fields = [\n",
        "        margs.dataset_name,\n",
        "        margs.measure_name,\n",
        "        pos_emb_type,\n",
        "        margs.fold,\n",
        "        margs.mode,\n",
        "        \"lr\",\n",
        "        margs.lr_start,\n",
        "        \"batch\",\n",
        "        margs.batch_size,\n",
        "        \"drop\",\n",
        "        margs.dropout,\n",
        "        margs.dims,\n",
        "    ]\n",
        "\n",
        "    run_name = \"_\".join(map(str, run_name_fields)) # This line is really the only reason run_name_fields was made\n",
        "\n",
        "    print(run_name)\n",
        "    datamodule = PropertyPredictionDataModule(margs)\n",
        "    margs.dataset_names = \"valid test\".split() # Funny way of splitting validation and test sets\n",
        "    margs.run_name = run_name\n",
        "    # Really un-deep set of lines\n",
        "    checkpoints_folder = margs.checkpoints_folder\n",
        "    checkpoint_root = os.path.join(checkpoints_folder, margs.measure_name)\n",
        "    margs.checkpoint_root = checkpoint_root\n",
        "    margs.run_id=np.random.randint(30000)\n",
        "    os.makedirs(checkpoints_folder, exist_ok=True)\n",
        "    checkpoint_dir = os.path.join(checkpoint_root, \"models_\"+str(margs.run_id))\n",
        "    results_dir = os.path.join(checkpoint_root, \"results\")\n",
        "    margs.results_dir = results_dir\n",
        "    margs.checkpoint_dir = checkpoint_dir\n",
        "    os.makedirs(results_dir, exist_ok=True)\n",
        "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "    checkpoint_path = os.path.join(checkpoints_folder, margs.measure_name)\n",
        "    checkpoint_callback = pl.callbacks.ModelCheckpoint(period=1, save_last=True, dirpath=checkpoint_dir, filename='checkpoint', verbose=True)\n",
        "\n",
        "    print(margs)\n",
        "    # Log it\n",
        "    logger = TensorBoardLogger(\n",
        "        save_dir=checkpoint_root,\n",
        "        #version=run_name,\n",
        "        name=\"lightning_logs\",\n",
        "        default_hp_metric=False,\n",
        "    )\n",
        "\n",
        "    tokenizer = MolTranBertTokenizer('bert_vocab.txt')\n",
        "    seed.seed_everything(margs.seed)\n",
        "\n",
        "    if margs.seed_path == '':\n",
        "        print(\"# training from scratch\")\n",
        "        model = LightningModule(margs, tokenizer)\n",
        "    else:\n",
        "        print(\"# loaded pre-trained model from {args.seed_path}\")\n",
        "        model = LightningModule(margs, tokenizer).load_from_checkpoint(margs.seed_path, strict=False, config=margs, tokenizer=tokenizer, vocab=len(tokenizer.vocab))\n",
        "\n",
        "\n",
        "    last_checkpoint_file = os.path.join(checkpoint_dir, \"last.ckpt\")\n",
        "    resume_from_checkpoint = None\n",
        "    if os.path.isfile(last_checkpoint_file):\n",
        "        print(f\"resuming training from : {last_checkpoint_file}\")\n",
        "        resume_from_checkpoint = last_checkpoint_file\n",
        "    else:\n",
        "        print(f\"training from scratch\")\n",
        "\n",
        "    trainer = pl.Trainer(\n",
        "        max_epochs=margs.max_epochs,\n",
        "        default_root_dir=checkpoint_root,\n",
        "        gpus=1,\n",
        "        logger=logger,\n",
        "        resume_from_checkpoint=resume_from_checkpoint,\n",
        "        checkpoint_callback=checkpoint_callback,\n",
        "        num_sanity_val_steps=0,\n",
        "    )\n",
        "\n",
        "    tic = time.perf_counter()\n",
        "    trainer.fit(model, datamodule)\n",
        "    toc = time.perf_counter()\n",
        "    print('Time was {}'.format(toc - tic))\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    }
  ]
}